from dotenv import load_dotenv
load_dotenv()

import os
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ğŸ”‘ Load OpenAI API key from .env
openai_key = os.getenv("OPENAI_API_KEY")

# ğŸ“„ Path to the PDF file
PDF_PATH = "your_file.pdf"

# ğŸ“š Load the PDF document
loader = PyPDFLoader(PDF_PATH)
pages = loader.load()

# âœ‚ï¸ Split the document into smaller chunks
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
documents = splitter.split_documents(pages)

# ğŸ§  Prepare prompt, model and parser
prompt = PromptTemplate.from_template(
    "Answer the question based on the following document: {context}\n\nQuestion: {question}"
)
llm = ChatOpenAI(model="gpt-3.5-turbo", api_key=openai_key)
output_parser = StrOutputParser()

# ğŸ”— Create the chain
chain = prompt | llm | output_parser

# ğŸ§ª Interactive loop
print("âœ… Document loaded. Ask a question or type 'exit' to quit.")

while True:
    question = input("â“ Your question: ")
    if question.lower() in ["exit", "quit", "q"]:
        break
    response = chain.invoke({"context": documents, "question": question})
    print("ğŸ§  AI Answer:", response)